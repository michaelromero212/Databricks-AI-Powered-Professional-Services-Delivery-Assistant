# Databricks AI-Powered Professional Services Delivery Assistant

![CI](https://github.com/michaelromero212/Databricks-AI-Powered-Professional-Services-Delivery-Assistant/actions/workflows/ci.yml/badge.svg)
![Python](https://img.shields.io/badge/python-3.10+-blue.svg)
![React](https://img.shields.io/badge/react-18+-61dafb.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

A professional internal tooling proof-of-concept designed for Databricks Professional Services (PS) teams. This application provides a unified layer for managing engagements, surfacing delivery risks through AI, and tracking the adoption of AI-enhanced workflows.

## üöÄ Overview

The **PS Delivery Assistant** bridges the gap between raw delivery data and actionable intelligence. By integrating Databricks-managed data with state-of-the-art LLMs, it empowers delivery managers and architects to identify blockers before they impact customer success.

### Key Capabilities
- **Unified Visibility**: Single pane of glass for all active PS engagements.
- **AI-Driven Insights**: Automated health summaries, risk detection, and thematic analysis of delivery notes.
- **Adoption Analytics**: Tracking the impact of AI tools on delivery velocity and time saved.
- **Databricks Native**: Designed to work seamlessly with Delta Lake and Databricks SQL.

---

## Screenshots

### Dashboard
A high-level overview of delivery health, active engagements, and daily AI usage trends.

![Dashboard](/docs/screenshots/dashboard.png)

---

### Engagements
A comprehensive list of projects with real-time health bars, status tracking, and contract value visibility.

![Engagements](/docs/screenshots/engagements.png)

---

### AI Insights
Interface for generating deep-dive analysis. Managers can select specific engagement notes to produce summarized risks and next actions.

![AI Insights](/docs/screenshots/ai_insights.png)

---

### Adoption Metrics
Visual evidence of tool utility, showing insights generated by role and total estimated hours saved across the team.

![Adoption Metrics](/docs/screenshots/adoption_metrics.png)

---

### Data Pipeline Workflow
A guided 3-step workflow for syncing data to Databricks and triggering notebook jobs directly from the UI.

![Data Pipeline Workflow](/docs/screenshots/data_pipeline_workflow.jpg)

**Features:**
- **Step 1**: Generate sample data or upload custom JSON files locally
- **Step 2**: Sync data to Unity Catalog Volumes with one click
- **Step 3**: Trigger Databricks notebooks (Data Ingestion, Feature Engineering, AI Insights, Metrics Tracking)
- Real-time job status monitoring with auto-refresh

---

## üõ†Ô∏è Technology Stack

| Component | Technology | Description |
|-----------|------------|-------------|
| **Frontend** | React + Vite | Fast, modern UI with an enterprise aesthetic. |
| **Backend** | FastAPI | High-performance Python API with asynchronous support. |
| **AI Layer** | Hugging Face | Mistral-7B-Instruct via the official `huggingface_hub` client. |
| **Databricks** | SDK + Jobs API | Unity Catalog Volumes for storage, Jobs API for notebook runs. |
| **Charts** | Plotly | Interactive, professional-grade data visualizations. |
| **Data** | Delta Lake / JSON | Production data in Databricks; local JSON cache for development. |
| **Styling** | Vanilla CSS | Custom design system optimized for readability and data density. |

---

## üèóÔ∏è Architecture

The application follows a modular tiered architecture:
1. **Data Layer**: Databricks notebooks automate ingestion from operational sources into Delta tables.
2. **AI Layer**: Hugging Face Inference Providers handle the heavy lifting of LLM processing.
3. **Application Layer**: FastAPI serves as the orchestrator, connecting the UI to data and AI services.
4. **Enablement Layer**: Usage metrics are persisted back to Databricks to measure ROI.

Detailed documentation can be found in [docs/architecture.md](docs/architecture.md).

---

## üèÅ Getting Started

### Prerequisites
- Python 3.8+
- Node.js 18+
- Hugging Face API Token

### Quick Start
1. **Clone & Setup**:
   ```bash
   pip install -r requirements.txt
   npm install --prefix app/frontend
   ```
2. **Environment**: Configure `.env` with your `HUGGINGFACE_API_TOKEN`.
3. **Run Stack**:
   - Backend: `python app/backend/api.py`
   - Frontend: `npm run dev --prefix app/frontend`

For a complete walkthrough, see the [Onboarding Guide](docs/onboarding.md).

---

## üê≥ Docker

Run the full stack with Docker Compose:

```bash
# Start backend and frontend
docker-compose up

# Or run backend only
docker build -f Dockerfile.backend -t ps-assistant .
docker run -p 8000:8000 ps-assistant
```

---

## üß™ Testing

Run the test suite:

```bash
# Install dependencies
pip install -r requirements.txt

# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ -v --cov=app/backend
```

**Test coverage:**
- `test_api.py` ‚Äî API endpoint tests
- `test_ai_client.py` ‚Äî AI/LLM integration tests  
- `test_connector.py` ‚Äî Data layer tests

---

## üîÑ CI/CD

GitHub Actions automatically runs on every push:
- ‚úÖ Backend tests (pytest)
- ‚úÖ Frontend build (npm)
- ‚úÖ Docker image verification

See [`.github/workflows/ci.yml`](.github/workflows/ci.yml) for the full pipeline.

---

## üìê Architecture Decisions

Key technical decisions are documented in [docs/ARCHITECTURE_DECISIONS.md](docs/ARCHITECTURE_DECISIONS.md):
- React + Vite over Streamlit
- Hugging Face over OpenAI
- FastAPI over Flask
- Unity Catalog Volumes over DBFS

---

*Created by the Databricks AI-Powered Delivery Team*
