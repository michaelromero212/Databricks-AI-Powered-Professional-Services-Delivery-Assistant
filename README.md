# Databricks AI-Powered Professional Services Delivery Assistant

A professional internal tooling proof-of-concept designed for Databricks Professional Services (PS) teams. This application provides a unified layer for managing engagements, surfacing delivery risks through AI, and tracking the adoption of AI-enhanced workflows.

## üöÄ Overview

The **PS Delivery Assistant** bridges the gap between raw delivery data and actionable intelligence. By integrating Databricks-managed data with state-of-the-art LLMs, it empowers delivery managers and architects to identify blockers before they impact customer success.

### Key Capabilities
- **Unified Visibility**: Single pane of glass for all active PS engagements.
- **AI-Driven Insights**: Automated health summaries, risk detection, and thematic analysis of delivery notes.
- **Adoption Analytics**: Tracking the impact of AI tools on delivery velocity and time saved.
- **Databricks Native**: Designed to work seamlessly with Delta Lake and Databricks SQL.

---

## Screenshots

### Dashboard
A high-level overview of delivery health, active engagements, and daily AI usage trends.

![Dashboard](/docs/screenshots/dashboard.png)

---

### Engagements
A comprehensive list of projects with real-time health bars, status tracking, and contract value visibility.

![Engagements](/docs/screenshots/engagements.png)

---

### AI Insights
Interface for generating deep-dive analysis. Managers can select specific engagement notes to produce summarized risks and next actions.

![AI Insights](/docs/screenshots/ai_insights.png)

---

### Adoption Metrics
Visual evidence of tool utility, showing insights generated by role and total estimated hours saved across the team.

![Adoption Metrics](/docs/screenshots/adoption_metrics.png)

---

### Data Pipeline Workflow
A guided 3-step workflow for syncing data to Databricks and triggering notebook jobs directly from the UI.

![Data Pipeline Workflow](/docs/screenshots/data_pipeline_workflow.jpg)

**Features:**
- **Step 1**: Generate sample data or upload custom JSON files locally
- **Step 2**: Sync data to Unity Catalog Volumes with one click
- **Step 3**: Trigger Databricks notebooks (Data Ingestion, Feature Engineering, AI Insights, Metrics Tracking)
- Real-time job status monitoring with auto-refresh

---

## üõ†Ô∏è Technology Stack

| Component | Technology | Description |
|-----------|------------|-------------|
| **Frontend** | React + Vite | Fast, modern UI with an enterprise aesthetic. |
| **Backend** | FastAPI | High-performance Python API with asynchronous support. |
| **AI Layer** | Hugging Face | Mistral-7B-Instruct via the official `huggingface_hub` client. |
| **Databricks** | SDK + Jobs API | Unity Catalog Volumes for storage, Jobs API for notebook runs. |
| **Charts** | Plotly | Interactive, professional-grade data visualizations. |
| **Data** | Delta Lake / JSON | Production data in Databricks; local JSON cache for development. |
| **Styling** | Vanilla CSS | Custom design system optimized for readability and data density. |

---

## üèóÔ∏è Architecture

The application follows a modular tiered architecture:
1. **Data Layer**: Databricks notebooks automate ingestion from operational sources into Delta tables.
2. **AI Layer**: Hugging Face Inference Providers handle the heavy lifting of LLM processing.
3. **Application Layer**: FastAPI serves as the orchestrator, connecting the UI to data and AI services.
4. **Enablement Layer**: Usage metrics are persisted back to Databricks to measure ROI.

Detailed documentation can be found in [docs/architecture.md](docs/architecture.md).

---

## üèÅ Getting Started

### Prerequisites
- Python 3.8+
- Node.js 18+
- Hugging Face API Token

### Quick Start
1. **Clone & Setup**:
   ```bash
   pip install -r requirements.txt
   npm install --prefix app/frontend
   ```
2. **Environment**: Configure `.env` with your `HUGGINGFACE_API_TOKEN`.
3. **Run Stack**:
   - Backend: `python app/backend/api.py`
   - Frontend: `npm run dev --prefix app/frontend`

For a complete walkthrough, see the [Onboarding Guide](docs/onboarding.md).

---
*Created by the Databricks AI-Powered Delivery Team*
